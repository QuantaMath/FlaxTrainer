{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gholamhossin/local/labs/.labs/lib/python3.10/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    }
   ],
   "source": [
    "from FlaxTrainer.callbacks import mockedcallback\n",
    "from FlaxTrainer.trainer import TrainerModule\n",
    "\n",
    "from flax import linen as nn\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "import flax\n",
    "\n",
    "from jax import numpy as jnp\n",
    "import jax\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"./saved_models/\"\n",
    "\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "def create_data_loaders(*datasets : Sequence[data.Dataset],\n",
    "                        train : Union[bool, Sequence[bool]] = True,\n",
    "                        batch_size : int = 128,\n",
    "                        num_workers : int = 4,\n",
    "                        seed : int = 42):\n",
    "    \"\"\"\n",
    "    Creates data loaders used in JAX for a set of datasets.\n",
    "\n",
    "    Args:\n",
    "      datasets: Datasets for which data loaders are created.\n",
    "      train: Sequence indicating which datasets are used for\n",
    "        training and which not. If single bool, the same value\n",
    "        is used for all datasets.\n",
    "      batch_size: Batch size to use in the data loaders.\n",
    "      num_workers: Number of workers for each dataset.\n",
    "      seed: Seed to initialize the workers and shuffling with.\n",
    "    \"\"\"\n",
    "    loaders = []\n",
    "    if not isinstance(train, (list, tuple)):\n",
    "        train = [train for _ in datasets]\n",
    "    for dataset, is_train in zip(datasets, train):\n",
    "        loader = data.DataLoader(dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=is_train,\n",
    "                                 drop_last=is_train,\n",
    "                                 collate_fn=numpy_collate,\n",
    "                                 num_workers=num_workers,\n",
    "                                 persistent_workers=is_train,\n",
    "                                 generator=torch.Generator().manual_seed(seed))\n",
    "        loaders.append(loader)\n",
    "    return loaders\n",
    "\n",
    "\n",
    "\n",
    "def target_function(x):\n",
    "    return np.sin(x * 3.0)\n",
    "\n",
    "class RegressionDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, num_points, seed):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.x = rng.uniform(low=-2.0, high=2.0, size=num_points)\n",
    "        self.y = target_function(self.x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx:idx+1], self.y[idx:idx+1]\n",
    "\n",
    "train_set = RegressionDataset(num_points=1000, seed=42)\n",
    "val_set = RegressionDataset(num_points=200, seed=43)\n",
    "test_set = RegressionDataset(num_points=500, seed=44)\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_set, val_set, test_set,\n",
    "                                                            train=[True, False, False],\n",
    "                                                            batch_size=64)\n",
    "\n",
    "x = np.linspace(-2, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    hidden_dims : Sequence[int]\n",
    "    output_dim : int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, **kwargs):\n",
    "        for dims in self.hidden_dims:\n",
    "            x = nn.Dense(dims)(x)\n",
    "            x = nn.silu(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLPRegressor([128, 128], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressTrainer(TrainerModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def create_functions(self):\n",
    "        def mse_loss(params, apply_fn, batch):\n",
    "            x, y = batch\n",
    "            pred = apply_fn({'params': params}, x)\n",
    "            loss = ((pred - y) ** 2).mean()\n",
    "            return loss\n",
    "\n",
    "        def train_step(state, batch):\n",
    "            loss_fn = lambda params: mse_loss(params, state.apply_fn, batch)\n",
    "            loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            metrics = {'loss': loss}\n",
    "            return state, metrics\n",
    "\n",
    "        def eval_step(state, batch):\n",
    "            loss = mse_loss(state.params, state.apply_fn, batch)\n",
    "            return {'loss': loss}\n",
    "\n",
    "        return train_step, eval_step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gholamhossin/local/labs/.labs/lib/python3.10/site-packages/flax/linen/summary.py:406: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  leaves = jax.tree_leaves(pytree)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                    MLPRegressor Summary                     </span>\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> path         </span>┃<span style=\"font-weight: bold\"> outputs         </span>┃<span style=\"font-weight: bold\"> params                   </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Inputs       │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float64</span>[64,1] │                          │\n",
       "│              │ - train: True   │                          │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│ Dense_0      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64,128] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]       │\n",
       "│              │                 │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,128]   │\n",
       "│              │                 │                          │\n",
       "│              │                 │ <span style=\"font-weight: bold\">256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 KB)</span>             │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│ Dense_1      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64,128] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]       │\n",
       "│              │                 │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,128] │\n",
       "│              │                 │                          │\n",
       "│              │                 │ <span style=\"font-weight: bold\">16,512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(66.0 KB)</span>         │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│ Dense_2      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64,1]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1]         │\n",
       "│              │                 │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,1]   │\n",
       "│              │                 │                          │\n",
       "│              │                 │ <span style=\"font-weight: bold\">129 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(516 B)</span>              │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│ MLPRegressor │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64,1]   │                          │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│<span style=\"font-weight: bold\">              </span>│<span style=\"font-weight: bold\">           Total </span>│<span style=\"font-weight: bold\"> 16,897 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(67.6 KB)</span><span style=\"font-weight: bold\">         </span>│\n",
       "└──────────────┴─────────────────┴──────────────────────────┘\n",
       "<span style=\"font-weight: bold\">                                                             </span>\n",
       "<span style=\"font-weight: bold\">             Total Parameters: 16,897 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(67.6 KB)</span><span style=\"font-weight: bold\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                    MLPRegressor Summary                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mpath        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                  \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Inputs       │ - \u001b[2mfloat64\u001b[0m[64,1] │                          │\n",
       "│              │ - train: True   │                          │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│ Dense_0      │ \u001b[2mfloat32\u001b[0m[64,128] │ bias: \u001b[2mfloat32\u001b[0m[128]       │\n",
       "│              │                 │ kernel: \u001b[2mfloat32\u001b[0m[1,128]   │\n",
       "│              │                 │                          │\n",
       "│              │                 │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m             │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│ Dense_1      │ \u001b[2mfloat32\u001b[0m[64,128] │ bias: \u001b[2mfloat32\u001b[0m[128]       │\n",
       "│              │                 │ kernel: \u001b[2mfloat32\u001b[0m[128,128] │\n",
       "│              │                 │                          │\n",
       "│              │                 │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m         │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│ Dense_2      │ \u001b[2mfloat32\u001b[0m[64,1]   │ bias: \u001b[2mfloat32\u001b[0m[1]         │\n",
       "│              │                 │ kernel: \u001b[2mfloat32\u001b[0m[128,1]   │\n",
       "│              │                 │                          │\n",
       "│              │                 │ \u001b[1m129 \u001b[0m\u001b[1;2m(516 B)\u001b[0m              │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│ MLPRegressor │ \u001b[2mfloat32\u001b[0m[64,1]   │                          │\n",
       "├──────────────┼─────────────────┼──────────────────────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m16,897 \u001b[0m\u001b[1;2m(67.6 KB)\u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m│\n",
       "└──────────────┴─────────────────┴──────────────────────────┘\n",
       "\u001b[1m                                                             \u001b[0m\n",
       "\u001b[1m             Total Parameters: 16,897 \u001b[0m\u001b[1;2m(67.6 KB)\u001b[0m\u001b[1m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    Dense_0: {\n",
       "        bias: (128,),\n",
       "        kernel: (1, 128),\n",
       "    },\n",
       "    Dense_1: {\n",
       "        bias: (128,),\n",
       "        kernel: (128, 128),\n",
       "    },\n",
       "    Dense_2: {\n",
       "        bias: (1,),\n",
       "        kernel: (128, 1),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Solve conflict of check_val_every_n_epoch and num_epochs\n",
    "#mock = mockedcallback.MockedCallback(stop_train=False)\n",
    "trainer = MLPRegressTrainer(optimizer_hparams={'lr': 4e-3},\n",
    "                            logger_params={'base_log_dir': CHECKPOINT_PATH},                           \n",
    "                            check_val_every_n_epoch=5)\n",
    " #                           callbacks=[mock])\n",
    "\n",
    "state = trainer.init_model(mlp,exmp_input=next(iter(train_loader))[0:1])\n",
    "#print(state)\n",
    "jax.tree_map(lambda x: x.shape, state.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 50/50 [00:04<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0008829445578157902\n",
      "Validation loss: 0.0008724512881599367\n",
      "Test loss: 0.0007670423365198076\n"
     ]
    }
   ],
   "source": [
    "metrics, state = trainer.train_model(\n",
    "    mlp,\n",
    "    state,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=50\n",
    ")\n",
    "\n",
    "#print(state)\n",
    "print(f'Training loss: {metrics[\"train/loss\"]}')\n",
    "print(f'Validation loss: {metrics[\"val/loss\"]}')\n",
    "print(f'Test loss: {metrics[\"test/loss\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleRNN(nn.Module):\n",
    "    #hidden_size: int\n",
    "    #output_size: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, hidden):\n",
    "        x = jax.numpy.concatenate([inputs, hidden], axis=-1)\n",
    "        i2o = nn.Dense(20)(x)\n",
    "        i2o = nn.softmax(i2o)\n",
    "        i2h = nn.Dense(hidden.shape[-1])(hidden)\n",
    "        i2h = nn.relu(i2h)\n",
    "        return i2o, i2h\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simpleRNN()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srnn = simpleRNN()\n",
    "srnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/gholamhossin/local/labs/jaxutil/FlaxTrainer/examples/mlp/notebook.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gholamhossin/local/labs/jaxutil/FlaxTrainer/examples/mlp/notebook.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m variables \u001b[39m=\u001b[39m srnn\u001b[39m.\u001b[39minit(key, a, b)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gholamhossin/local/labs/jaxutil/FlaxTrainer/examples/mlp/notebook.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m jax\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mshape, variables)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'key' is not defined"
     ]
    }
   ],
   "source": [
    "class SRNNTrainer(TrainerModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def create_functions(self):\n",
    "        def mse_loss(params, apply_fn, batch):\n",
    "            x, y = batch\n",
    "            pred = apply_fn({'params': params}, x)\n",
    "            loss = ((pred - y) ** 2).mean()\n",
    "            return loss\n",
    "\n",
    "        def train_step(state, batch):\n",
    "            loss_fn = lambda params: mse_loss(params, state.apply_fn, batch)\n",
    "            loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            metrics = {'loss': loss}\n",
    "            return state, metrics\n",
    "\n",
    "        def eval_step(state, batch):\n",
    "            loss = mse_loss(state.params, state.apply_fn, batch)\n",
    "            return {'loss': loss}\n",
    "\n",
    "        return train_step, eval_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 2, 20), (3, 2, 3))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = srnn.apply(variables, a, b)\n",
    "jax.tree_map(lambda x: x.shape, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1, 2, 2, ..., 1, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(carry, x):\n",
    "     res = x*x\n",
    "     return res, res\n",
    "\n",
    "a = jax.random.randint(jax.random.PRNGKey(128), (10000,), minval=1, maxval=3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(1, dtype=int32),\n",
       " DeviceArray([1, 4, 4, ..., 1, 4, 1], dtype=int32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.lax.scan(f, jnp.array(0, dtype=jnp.int32), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1, 4, 4, ..., 1, 4, 1], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.power(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([    1,     3,     5, ..., 14991, 14993, 14994], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cumsum(res, el):\n",
    "    \"\"\"\n",
    "    - `res`: The result from the previous loop.\n",
    "    - `el`: The current array element.\n",
    "    \"\"\"\n",
    "    res = res + el\n",
    "    return res, res  # (\"carryover\", \"accumulated\")\n",
    "\n",
    "\n",
    "result_init = 0\n",
    "final, result = jax.lax.scan(cumsum, result_init, a)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([    1,     3,     5, ..., 14991, 14993, 14994], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.cumsum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(14994, dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = lambda res, el: (res+el, res+el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.labs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7c17e30e529363162b4f2bd4620f627c628f84c34e1cd85cef9b92e26a024c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
